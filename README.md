# GPT Transformer Decoder

This project is an implementation of a Transformer decoder architecture. It includes key components such as self-attention, causal masking, positional encoding, and token generation.

## Features

Decoder-only Transformer (GPT-style)

Causal self-attention

Token-by-token generation